# -*- coding: utf-8 -*-
"""diabetes_deep_learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DPLlv5kbdPOOKVsZlcT5c8eR50ipywF2

#Base de dados retidado do kaggle

https://www.kaggle.com/uciml/pima-indians-diabetes-database

O codigo a seguir foi construito somente para demonstração de conhecimento sem nehum uso profissional.

The following code was created only to demonstrate knowledge without professional use.

El siguiente código fue creado solo para demostrar conocimiento sin uso profesional.

#Etapa 1: Importação das bibliotecas
"""

import tensorflow as tf
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
tf.__version__

"""#Etapa 2: Carregar a base de dados"""

data = pd.read_csv('diabetes.csv')

data.shape

data.isnull().values.any()

data

data.info()

data.describe()

sns.countplot(x = 'Outcome', data = data);

data.hist(figsize=(20,12));

sns.pairplot(data, hue = 'Outcome', 
             vars = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']);

"""# Mapa de Correlação"""

sns.heatmap(data.corr(), annot = True);

"""#Etapa 3: Divisão dos dados 
Varial x busca os atributos de 0 até 7 exlcuindo a coluna outcome.
"""

X = data.iloc[:, 0:8].values

X

"""Variavel y trás somente o atributo 8 o *outcome*"""

y = data.iloc[:, 8].values

y

"""Utilizar o StandarScaler transforma os dados na mesma escala"""

#https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X = sc.fit_transform(X)

X

"""Divisão da base de treinamento e teste e utilizando 20% para teste no atributo teste_siza = 20"""

#https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)

"""Shape da base de treinamento"""

X_train.shape

"""Shape da base de teste"""

X_test.shape

"""#Etapa 4: Contrução e treinamento do modelo"""

# https://www.tensorflow.org/api_docs/python/tf/keras
# https://keras.io/api/
# https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/
# https://machinelearningmastery.com/how-to-avoid-exploding-gradients-in-neural-networks-with-gradient-clipping/

classifier = tf.keras.models.Sequential()
classifier.add(tf.keras.layers.Dense(units=400, activation='relu', kernel_initializer = 'random_uniform', input_shape=(8, )))
classifier.add(tf.keras.layers.Dropout(0.2))
classifier.add(tf.keras.layers.Dense(units=400, activation='relu', kernel_initializer = 'random_uniform'))
classifier.add(tf.keras.layers.Dropout(0.2))
classifier.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))

classifier.summary()

otimizador = tf.keras.optimizers.Adamax(lr = 0.001, decay = 0.0001, clipvalue = 0.5)

classifier.compile(optimizer = otimizador, loss = 'binary_crossentropy', metrics = ['binary_accuracy'])

epochs_hist = classifier.fit(X_train, y_train, batch_size = 30, epochs = 500)

"""#Etapa 5: Realizando as previsões"""

y_pred = classifier.predict(X_test)

y_pred

y_pred = (y_pred > 0.5)

y_pred

"""#Etapa 6: Avaliação do modelo"""

epochs_hist.history.keys()

plt.plot(epochs_hist.history['loss'])
plt.title('Model Loss Progress During Training')
plt.xlabel('Epoch')
plt.ylabel('Training and Validation Loss')
plt.legend(['Loss']);

plt.plot(epochs_hist.history['binary_accuracy'])
plt.title('Model accuracy Progress During Training')
plt.xlabel('Epoch')
plt.ylabel('Training and Validation accuracy')
plt.legend(['accuracy']);

"""Resultados base treinamento"""

from sklearn.metrics import confusion_matrix
y_train_pred = classifier.predict(X_train)
y_train_pred = (y_train_pred > 0.5)
cm = confusion_matrix(y_train, y_train_pred)
sns.heatmap(cm, annot=True);

cm

from sklearn.metrics import classification_report
print(classification_report(y_train_pred, y_train))

"""Resultados base de teste"""

cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True);

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))